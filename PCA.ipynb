{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###  PCA (Principal Component Analysis)\n",
        "**Principal Component Analysis (PCA)** is a **statistical and machine learning technique** used to **reduce the number of features (dimensions)** in a dataset while **preserving as much information (variance) as possible**\n",
        "It is mainly used when data has **many features**, which makes models slow, complex, or noisy.\n",
        "\n"
      ],
      "metadata": {
        "id": "VeapePOoHXdz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBWrDvZHFbgb"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# load iris data\n",
        "iris = load_iris()\n",
        "\n",
        "\n",
        "# call PCA\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# fit and transform the data\n",
        "iris_pca = pca.fit_transform(iris.data)\n",
        "\n",
        "# now we can plot the first two principle commponent\n",
        "plt.scatter(iris_pca[:,0], iris_pca[:,1], c=iris.target)\n",
        "plt.xlabel('First Principle Component')\n",
        "plt.ylabel('Second Principle Component')\n",
        "plt.title('PCA of Iris Dataset')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PCA on Tips Data**"
      ],
      "metadata": {
        "id": "ISSDgfTaKZkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# load dataset\n",
        "df = sns.load_dataset('tips')\n",
        "\n",
        "# preprocessing\n",
        "# encode the categorical data\n",
        "le = LabelEncoder()\n",
        "cat_feature = df.select_dtypes(include=['category']).columns\n",
        "for col in cat_feature:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# standardize the data\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "# call PCA\n",
        "pca = PCA()\n",
        "\n",
        "# fit and transform the data\n",
        "df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "\n",
        "# plot the explained variance ratio\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DOdjNnBhH8f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6JlzdyyLgSa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}